---
title: 'STAT 420: Quiz Assignment'
author: "Nico Kienawan, nicok2"
date: "4/30/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Collinearity
Consider the following model,
\[
Y = 4 + 3 X_1 + 2 X_2 + X_3 + \epsilon,
\]
where $\epsilon \sim N(\mu = 0, \sigma = 3)$.  
Simulate a sample size of 100 observations from this model.
```{r}
n = 100
set.seed(910)
x1 = runif(n, 0 , 10)
x2 = runif(n, 0 , 10)
y = function(x1, x2, x3) {4 + 3 * x1 + 2 * x2 + x3 + rnorm(n, 0 , 3)}
```
**(a)** Let $X_3 = 6 X_1 + 5 X_2$, save this as `x3_a`. Using `lm()`, fit a simple linear model and save the model as `model_a`. Then, print the `summary()` of the model. Describe anything unusual from the `summary()`.
```{r}
x3_a = 6 * x1 + 5 * x2
y_a = y(x1, x2, x3_a)
model_a = lm(y_a ~ x1 + x2 + x3_a)
summary(model_a)
```
The values in the `x3` row are all NAs.

**(b)** Now, let $X_3 = 6 X_1 + 5 X_2 + \epsilon$ where $\epsilon \sim N(\mu = 0, \sigma = 0.1)$, save this as `x3_b`. Using `lm()`, fit a simple linear model and save the model as `model_b`. Then, print the `summary()` of the model. Report the R-squared and the p-value of each predictors. Do you see any differences from the `summary()` in part **(a)**? Why are they different?
```{r}
x3_b = 6 * x1 + 5 * x2 + rnorm(n, 0, 0.1)
y_b = y(x1, x2, x3_b)
model_b = lm(y_b ~ x1 + x2 + x3_b)
summary(model_b)
summary(model_b)$r.squared
summary(model_b)$coefficient[2:4,4]
```
The values in the `x3` row are not NAs anymore.  
They are different because the `x3` in `model_a` is exactly linearly dependent to `x1` and `x2`. While in `model_b`, the `x3` is still linearly dependent to `x1` and `x2` but not exactly.

**(c)** Notice that the R-squared in `model_b` is very close to 1. However, none of the predictors are significant. Why do you think this happens?  
This happens because the predictors in `model_b` is highly correlated (`x3` is very linearly dependent to `x1` and `x2` with the $\sigma$ of $\epsilon$ only 0.1) which reduces the significance of `x1` and `x2`.

**(d)** Suppose `X = cbind(1, x1, x2, x3_b)`. Find the eigenvalues of $X^TX$. Report the smallest eigenvalue. What do you think is the relationship between this eigenvalue and the significance of the t-test above? (Remember: $(X^TX)^{-1} = QD^{-1}Q^T$ where $D$ is a diagonal matrix with the eigenvalues of $X^TX$ and variance of $\hat\beta$ is $\hat\sigma^2(X^TX)^{-1}$)
```{r}
x = cbind(1, x1, x2, x3_b)
eigenval = eigen(t(x) %*% x)$values
eigenval
eigenval[which.min(eigenval)]
```
Since an eigenvalue of $X^TX$ is very small, $(X^TX)^{-1}$ will be very large because $(X^TX)^{-1} = QD^{-1}Q^T$.  
If $(X^TX)^{-1}$ is very large, the variance of $\hat\beta$ will be very large too because $Var(\hat\beta) = \hat\sigma^2(X^TX)^{-1}$ and $\hat\sigma^2$ is around 1.  
Since the variance of $\hat\beta$ is very large, the confidence interval of the test will also be large, which makes the t-test above not significant.

**(e)** Find the Variance Inflation Factor (VIF) of `model_b`. Do you find any "problematic" predictors? What does it mean?
```{r}
library(car)
vif(model_b)
```
Yes, all of the predictors are "problematic" because the VIFs are very large.  
It means there are correlations between the predictors.

# Appendix 
- The Collinearity problem used the Multicollinearity_SLIDES powerpoint as a reference.